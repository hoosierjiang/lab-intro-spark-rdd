{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a connection from your Jupyter Notebook to the Spark cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the SparkContext and/or SparkSession objects to connect to the Spark Cluster.**\n",
    "\n",
    "Since the Jupyter notebooks are using the Python 3 Kernel, you will need to \"connect\" to the cluster in each Jupyter notebook to run the code. This was done for the sake of simplicity, and the notebooks provided in this Lab/Assignment should work without issue. However, should you run these notebooks in another platform, they may not work. Remember, the spark \"driver\" is running within this Jupyter notebook.\n",
    "\n",
    "You have to use functions from the `pyspark` module, and to connect to the cluster in this workbook you can run the following cell. You may want to copy this cell to other workbooks you start, and you can change the `appName`.\n",
    "\n",
    "Also, in this configuration, be sure to import `findspark` and run `findspark.init()` so that the Python session can find pyspark and the Spark variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SparkContext if you are just working with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SparkSession if you are just working with Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "     .appName(\"Test SparkSession\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to access the SparkContext associated with the SparkSession `spark`, define it after creating the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext      # get the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!: Close the `sc` or `spark` connection before you close your notebook.**\n",
    "\n",
    "Just as you have to crete the connection object, you need to shut down the connection before you exit the Jupyter notebook. If you do not do this, then you will have orphan connection to the cluster which will have asked for resources and not released them. Remember, Spark uses memory, so if the memorys isn't available when you create a connection object, then Spark will not perform.\n",
    "\n",
    "Closing the connection is very easy. (Do not run the following cell unless you want to close the connection). This cell has not been added to the example notebooks, it is up to you do do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of SparkContext and SparkSession\n",
    "\n",
    "** Note: This section was taken verbatim from the book [Fast Data Processing with Spark - 3ed](https://www.packtpub.com/big-data-and-business-intelligence/fast-data-processing-spark-2-third-edition).**\n",
    "\n",
    "Prior to Spark 2.0.0, the three main connection objects were `SparkContext, SqlContext, and HiveContext`. The `SparkContext` object was the connection to a Spark execution environment and created RDDs and others, `SQLContext` worked with `SparkSQL` in the background of `SparkContext`, and `HiveContext` interacted with the Hive stores.\n",
    "\n",
    "Spark 2.0.0 introduced Datasets/DataFrames as the main distributed data abstraction interface and the `SparkSession` object as the entry point to a Spark execution environment. Appropriately, the `SparkSession` object is found in the namespace, `org.apache.spark.sql.SparkSession` (Scala), or `pyspark.sql.sparkSession`. A few points to note are as follows:\n",
    "\n",
    "* In Scala and Java, Datasets form the main data abstraction as typed data; however, for Python and R (which do not have compile time type checking), the data abstraction is DataFrame. For all practical API purposes, the Datasets in Scala/Java are the same as DataFrames in Python/R.\n",
    "* While Datasets/DataFrames are top-level interfaces, RDDs have not disappeared. In fact, the underlying structures are still RRDs. Also, to interact with RDDs, we still need a SparkContext object and we can get one from the SparkSession object.\n",
    "* The `SparkSession` object encapsulates the `SparkContext` object. As of version 2.0.0, `SparkContext` is still the conduit to a Spark cluster (local or remote)\n",
    "\n",
    "However, for operations such as reading and creating Datasets, use the `SparkSession` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see some of the attributes associated with the `SparkSession` and `SparkContext` you can run some methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that your cluster is connected to a cluster using YARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "print(conf.toDebugString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
